{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP2nN5yQa32oCf9WOYQ9VD5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Gsz8qOeDcRWm","executionInfo":{"status":"ok","timestamp":1713257013056,"user_tz":-120,"elapsed":11260,"user":{"displayName":"Jan Zdrazil","userId":"08088439700332738004"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from scipy.ndimage import gaussian_filter\n","import random\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","from PIL import Image\n","\n","import albumentations as A\n","from albumentations.core.transforms_interface import DualTransform, ImageOnlyTransform\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5j3jQYDbdo9b","executionInfo":{"status":"ok","timestamp":1713257029682,"user_tz":-120,"elapsed":16632,"user":{"displayName":"Jan Zdrazil","userId":"08088439700332738004"}},"outputId":"d1b01f1d-a71c-4b46-c7c7-f40934d0f781"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["# ShowCase of Plant Detection Augmentations"],"metadata":{"id":"1PCaYlPBhGjt"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, csv_path, image_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.annotations = pd.read_csv(csv_path)\n","        self.grouped_annotations = self._group_annotations(self.annotations)\n","        self.transform = transform\n","\n","    def _group_annotations(self, dataframe):\n","        grouped = {}\n","        for _, row in dataframe.iterrows():\n","            image_name = row['image_name']\n","            image_path = os.path.join(self.image_dir, image_name)\n","\n","            if os.path.isfile(image_path):\n","                label = row['label_name']\n","                bbox = [row['bbox_x'], row['bbox_y'], row['bbox_x'] + row['bbox_width'], row['bbox_y'] + row['bbox_height']]\n","\n","                if image_name in grouped:\n","                    grouped[image_name]['boxes'].append(bbox)\n","                    grouped[image_name]['labels'].append(label)\n","                else:\n","                    grouped[image_name] = {\n","                        'boxes': [bbox],\n","                        'labels': [label]\n","                    }\n","        return grouped\n","\n","    def __len__(self):\n","        return len(self.grouped_annotations)\n","\n","    def __getitem__(self, idx):\n","        image_name = sorted(self.grouped_annotations.keys())[idx]\n","        image_path = os.path.join(self.image_dir, image_name)\n","        image = Image.open(image_path).convert(\"RGB\")\n","\n","        annotations = self.grouped_annotations[image_name]\n","        boxes = annotations['boxes']\n","        labels = annotations['labels']\n","        class_mapping = {'plant': 1}\n","        labels = [class_mapping.get(label, 0) for label in labels]\n","\n","        if self.transform:\n","            image = np.array(image)\n","            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n","            image = transformed['image']\n","            boxes = transformed['bboxes']\n","            labels = transformed['labels']\n","        else:\n","            image = torch.as_tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1) / 255.0\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","\n","        target = {\"boxes\": boxes, \"labels\": labels}\n","\n","        return image, target"],"metadata":{"id":"LIck01nOdpAP","executionInfo":{"status":"ok","timestamp":1713259367345,"user_tz":-120,"elapsed":336,"user":{"displayName":"Jan Zdrazil","userId":"08088439700332738004"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["def augmentations1():\n","    return A.Compose([\n","        A.Blur(blur_limit=4, p=1),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n","\n","def augmentations2():\n","    return A.Compose([\n","        A.RandomBrightnessContrast(p=1),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n","\n","def augmentations3():\n","    return A.Compose([\n","        A.Sharpen(p=1),\n","        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ToTensorV2()\n","    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))"],"metadata":{"id":"_BGLMLpTdpCt","executionInfo":{"status":"ok","timestamp":1713259514569,"user_tz":-120,"elapsed":6,"user":{"displayName":"Jan Zdrazil","userId":"08088439700332738004"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# Paths to your CSV and images\n","csv = '/content/gdrive/MyDrive/plant_detection/LabelsToTrain/plant_train_data1_3.csv'\n","img_path = '/content/gdrive/MyDrive/plant_detection/DataToTrain'\n","\n","\n","# Instantiate your custom datasets without transforms\n","aug1 = CustomDataset(csv_path=csv, image_dir=img_path, transform=augmentations1())\n","aug2 = CustomDataset(csv_path=csv, image_dir=img_path, transform=augmentations2())\n","aug3 = CustomDataset(csv_path=csv, image_dir=img_path, transform=augmentations3())"],"metadata":{"id":"yYh9BsVcdpFZ","executionInfo":{"status":"ok","timestamp":1713259518698,"user_tz":-120,"elapsed":4133,"user":{"displayName":"Jan Zdrazil","userId":"08088439700332738004"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["def plot_images_with_boxes(loaders, titles):\n","    fig, axs = plt.subplots(1, len(loaders), figsize=(25, 16))\n","\n","    for idx, loader in enumerate(loaders):\n","        image, target = next(iter(loader))\n","\n","        image = image.permute(1, 2, 0).cpu().numpy()\n","\n","        ax = axs[idx]\n","        ax.imshow(image)\n","        ax.set_title(titles[idx])\n","\n","        for box in target['boxes']:\n","            box = box.cpu().numpy()\n","            x, y, x_max, y_max = box\n","            width = x_max - x\n","            height = y_max - y\n","            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n","            ax.add_patch(rect)\n","            ax.text(x, y, 'Plant', color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n","        ax.axis('off')\n","\n","    plt.show()"],"metadata":{"id":"f8e9P5XXdpKt","executionInfo":{"status":"ok","timestamp":1713259518698,"user_tz":-120,"elapsed":7,"user":{"displayName":"Jan Zdrazil","userId":"08088439700332738004"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["loaders = [aug1, aug2, aug3]\n","titles = ['Blur', 'RandomBrightness', 'Sharpen']\n","\n","plot_images_with_boxes(loaders, titles)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AyxALnVkdpNX","outputId":"94142993-b82f-47e6-edda-a7cd3a414c4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AxQwAE2xOYtW"},"execution_count":null,"outputs":[]}]}